{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic modelling\n",
    "## Unsupervised learning of topics in a text\n",
    "### using Latent Dirchlet Allocation (via sklearn)\n",
    "Topic modelling can be thought of as dimensionality reduction:  \n",
    "Documents are represented as sets of topics  \n",
    "Each topic has a weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/secretstorage/dhcrypto.py:15: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/usr/lib/python3/dist-packages/secretstorage/util.py:19: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "Requirement already satisfied: scikit-learn==0.22.2.post1 in ./.local/lib/python3.8/site-packages (0.22.2.post1)\n",
      "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn==0.22.2.post1) (1.8.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn==0.22.2.post1) (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn==0.22.2.post1) (1.20.3)\n",
      "/usr/lib/python3/dist-packages/secretstorage/dhcrypto.py:15: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/usr/lib/python3/dist-packages/secretstorage/util.py:19: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "Requirement already satisfied: nltk==3.6.2 in ./.local/lib/python3.8/site-packages (3.6.2)\n",
      "Requirement already satisfied: click in ./.local/lib/python3.8/site-packages (from nltk==3.6.2) (8.0.4)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk==3.6.2) (1.1.0)\n",
      "Requirement already satisfied: regex in ./.local/lib/python3.8/site-packages (from nltk==3.6.2) (2022.9.13)\n",
      "Requirement already satisfied: tqdm in ./.local/lib/python3.8/site-packages (from nltk==3.6.2) (4.64.1)\n",
      "/usr/lib/python3/dist-packages/secretstorage/dhcrypto.py:15: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/usr/lib/python3/dist-packages/secretstorage/util.py:19: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "Requirement already satisfied: pandas==1.2.4 in ./.local/lib/python3.8/site-packages (1.2.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas==1.2.4) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.8/dist-packages (from pandas==1.2.4) (1.20.3)\n",
      "Requirement already satisfied: pytz>=2017.3 in ./.local/lib/python3.8/site-packages (from pandas==1.2.4) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7.3->pandas==1.2.4) (1.14.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/mjams001/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn==0.22.2.post1\n",
    "!pip install nltk==3.6.2\n",
    "!pip install pandas==1.2.4\n",
    "import re\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import csv\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import string\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use CountVectorizer to turn the docs into vectors\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the stemmer\n",
    "stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "stopwords_file_path = 'en_stopwords.csv'\n",
    "def read_in_csv(csv_file):\n",
    "    with open(csv_file, 'r', encoding='utf-8') as fp:\n",
    "        reader = csv.reader(fp, delimiter=',', quotechar='\"')\n",
    "        data_read = [row for row in reader]\n",
    "    return data_read\n",
    "\n",
    "def get_stopwords(path=stopwords_file_path):\n",
    "    stopwords = read_in_csv(path)\n",
    "    stopwords = [word[0] for word in stopwords]\n",
    "    stemmed_stopwords = [stemmer.stem(word) for word in stopwords]\n",
    "    stopwords = stopwords + stemmed_stopwords\n",
    "    return stopwords\n",
    "\n",
    "def tokenize_and_stem(sentence):\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    filtered_tokens = [t for t in tokens if t not in stopwords and t not in string.punctuation and re.search('[a-zA-Z]', t)]\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weâ€™ll use a public dataset from the BBC comprised of 2,225 articles  \n",
    "Each labeled under one of 5 categories: business, entertainment, politics, sport or tech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in our data\n",
    "stopwords_file_path = \"en_stopwords.csv\"\n",
    "stopwords = get_stopwords(stopwords_file_path)\n",
    "bbc_dataset = \"en_stopwords.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn the documents into vectors\n",
    "def create_count_vectorizer(documents):\n",
    "    count_vectorizer = CountVectorizer(stop_words=stopwords, tokenizer=tokenize_and_stem, max_features=1500)\n",
    "    data = count_vectorizer.fit_transform(documents)\n",
    "    return (count_vectorizer, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove unwanted characters (keep just words and spaces)\n",
    "def clean_data(df):\n",
    "    df['description'] = df['description'].apply(lambda x: re.sub(r'[^\\w\\s]', ' ', x))\n",
    "    df['description'] = df['description'].apply(lambda x: re.sub(r'\\d', '', x))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the LDA model (note that usually num_topics is unknown)\n",
    "def create_and_fit_lda(data, num_topics):\n",
    "    lda = LDA(n_components=num_topics, n_jobs=-1)\n",
    "    lda.fit(data)\n",
    "    return lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify & print the most common topic words\n",
    "def get_most_common_words_for_topics(model, vectorizer, n_top_words):\n",
    "    words = vectorizer.get_feature_names()\n",
    "    word_dict = {}\n",
    "    for topic_index, topic in enumerate(model.components_):\n",
    "        this_topic_words = [words[i] for i in topic.argsort()[:-n_top_words - 1:-1]]\n",
    "        word_dict[topic_index] = this_topic_words\n",
    "    return word_dict\n",
    "\n",
    "def print_topic_words(word_dict):\n",
    "    for key in word_dict.keys():\n",
    "        print(f\"Topic {key}\")\n",
    "        print(\"\\t\", word_dict[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the data, clean it, get text\n",
    "df = pd.read_csv(bbc_dataset)\n",
    "df = clean_data(df)\n",
    "documents = df['description']\n",
    "\n",
    "# set number of topics (note that usually this is unknown)\n",
    "number_topics = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>name</th>\n",
       "      <th>size</th>\n",
       "      <th>source_url</th>\n",
       "      <th>date</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>_none.txt</td>\n",
       "      <td>_None_</td>\n",
       "      <td>0</td>\n",
       "      <td>en/_none.txt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No stop word removal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sphinx.txt</td>\n",
       "      <td>Sphinx</td>\n",
       "      <td>0</td>\n",
       "      <td>http://sphinxsearch.com/docs/current.html#conf...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sphinx is an open source search server  Top go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ebscohost_medline_cinahl.txt</td>\n",
       "      <td>EBSCOhost</td>\n",
       "      <td>24</td>\n",
       "      <td>https://help.ebsco.com/interfaces/CINAHL_MEDLI...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The stop words used in EBSCOhost medical datab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>corenlp_hardcoded.txt</td>\n",
       "      <td>CoreNLP (Hardcoded)</td>\n",
       "      <td>28</td>\n",
       "      <td>https://github.com/stanfordnlp/CoreNLP/blob/ma...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hardcoded in src edu stanford nlp coref data W...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ranksnl_oldgoogle.txt</td>\n",
       "      <td>Ranks NL (Google)</td>\n",
       "      <td>32</td>\n",
       "      <td>http://www.ranks.nl/stopwords</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The short stopwords list below is based on wha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>ranksnl_large.txt</td>\n",
       "      <td>Ranks NL (Large)</td>\n",
       "      <td>667</td>\n",
       "      <td>http://www.ranks.nl/stopwords</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A very long list from ranks nl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>tonybsk_6.txt</td>\n",
       "      <td>tonybsk_6.txt</td>\n",
       "      <td>671</td>\n",
       "      <td>https://github.com/igorbrigadir/stopwords/blob...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Unknown origin   I lost the reference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>terrier.txt</td>\n",
       "      <td>Terrier</td>\n",
       "      <td>733</td>\n",
       "      <td>http://terrier.org/docs/v4.1/javadoc/org/terri...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Terrier Retrieval Engine  Stopword list to loa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>atire_puurula.txt</td>\n",
       "      <td>ATIRE (Puurula)</td>\n",
       "      <td>988</td>\n",
       "      <td>http://www.atire.org/hg/atire/file/tip/source/...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Included in ATIRE See  Paper  http   www aclwe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>alir3z4.txt</td>\n",
       "      <td>Alir3z4</td>\n",
       "      <td>1298</td>\n",
       "      <td>https://github.com/Alir3z4/stop-words/blob/mas...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>List of common stop words in various languages...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>68 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            file                 name  size  \\\n",
       "0                      _none.txt               _None_     0   \n",
       "1                     sphinx.txt               Sphinx     0   \n",
       "2   ebscohost_medline_cinahl.txt            EBSCOhost    24   \n",
       "3          corenlp_hardcoded.txt  CoreNLP (Hardcoded)    28   \n",
       "4          ranksnl_oldgoogle.txt    Ranks NL (Google)    32   \n",
       "..                           ...                  ...   ...   \n",
       "63             ranksnl_large.txt     Ranks NL (Large)   667   \n",
       "64                 tonybsk_6.txt        tonybsk_6.txt   671   \n",
       "65                   terrier.txt              Terrier   733   \n",
       "66             atire_puurula.txt      ATIRE (Puurula)   988   \n",
       "67                   alir3z4.txt              Alir3z4  1298   \n",
       "\n",
       "                                           source_url  date  \\\n",
       "0                                        en/_none.txt   NaN   \n",
       "1   http://sphinxsearch.com/docs/current.html#conf...   NaN   \n",
       "2   https://help.ebsco.com/interfaces/CINAHL_MEDLI...   NaN   \n",
       "3   https://github.com/stanfordnlp/CoreNLP/blob/ma...   NaN   \n",
       "4                       http://www.ranks.nl/stopwords   NaN   \n",
       "..                                                ...   ...   \n",
       "63                      http://www.ranks.nl/stopwords   NaN   \n",
       "64  https://github.com/igorbrigadir/stopwords/blob...   NaN   \n",
       "65  http://terrier.org/docs/v4.1/javadoc/org/terri...   NaN   \n",
       "66  http://www.atire.org/hg/atire/file/tip/source/...   NaN   \n",
       "67  https://github.com/Alir3z4/stop-words/blob/mas...   NaN   \n",
       "\n",
       "                                          description  \n",
       "0                               No stop word removal   \n",
       "1   Sphinx is an open source search server  Top go...  \n",
       "2   The stop words used in EBSCOhost medical datab...  \n",
       "3   Hardcoded in src edu stanford nlp coref data W...  \n",
       "4   The short stopwords list below is based on wha...  \n",
       "..                                                ...  \n",
       "63                     A very long list from ranks nl  \n",
       "64             Unknown origin   I lost the reference   \n",
       "65  Terrier Retrieval Engine  Stopword list to loa...  \n",
       "66  Included in ATIRE See  Paper  http   www aclwe...  \n",
       "67  List of common stop words in various languages...  \n",
       "\n",
       "[68 rows x 6 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Extract one of the categories  \n",
    "Select a particular category from the dataframe, e.g. tech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                 No stop word removal \n",
       "1     Sphinx is an open source search server  Top go...\n",
       "2     The stop words used in EBSCOhost medical datab...\n",
       "3     Hardcoded in src edu stanford nlp coref data W...\n",
       "4     The short stopwords list below is based on wha...\n",
       "                            ...                        \n",
       "63                       A very long list from ranks nl\n",
       "64               Unknown origin   I lost the reference \n",
       "65    Terrier Retrieval Engine  Stopword list to loa...\n",
       "66    Included in ATIRE See  Paper  http   www aclwe...\n",
       "67    List of common stop words in various languages...\n",
       "Name: description, Length: 68, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.loc[df['category']=='tech']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create vectorizer & model\n",
    "(vectorizer, data) = create_count_vectorizer(documents)\n",
    "lda = create_and_fit_lda(data, number_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "                          evaluate_every=-1, learning_decay=0.7,\n",
       "                          learning_method='batch', learning_offset=10.0,\n",
       "                          max_doc_update_iter=100, max_iter=10,\n",
       "                          mean_change_tol=0.001, n_components=5, n_jobs=-1,\n",
       "                          perp_tol=0.1, random_state=None,\n",
       "                          topic_word_prior=None, total_samples=1000000.0,\n",
       "                          verbose=0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Inspect the results  \n",
    "Are they coherent? Do they seem to be different topics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0\n",
      "\t ['word', 'the', 'and', 'are', 'stopword', 'search', 'in', 'stop', 'of', 'a']\n",
      "Topic 1\n",
      "\t ['list', 'to', 'smart', 'also', 'same', 'as', 'and', 'bow', 'an', 'edu']\n",
      "Topic 2\n",
      "\t ['for', 'a', 'list', 'paper', 'in', 'www', 'use', 'http', 'includ', 'time']\n",
      "Topic 3\n",
      "\t ['stopword', 'list', 'snowbal', 'http', 'sphinx', 'postgresql', 'for', 'english', 'from', 'to']\n",
      "Topic 4\n",
      "\t ['the', 'in', 'stopword', 'list', 'use', 'and', 'word', 'is', 'of', 'a']\n"
     ]
    }
   ],
   "source": [
    "# inspect the contents of the topics\n",
    "topic_words = get_most_common_words_for_topics(lda, vectorizer, 10)\n",
    "print_topic_words(topic_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "def test_new_example(lda, vect, example):\n",
    "    vectorized = vect.transform([example])\n",
    "    topic = lda.transform(vectorized)\n",
    "    print(topic)\n",
    "    return topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = \"\"\" what I would encourage you to do.I have actually run this several times and seen much more convincingoutputs than this, so it does vary.I definitely would encourage you to try this for yourself.Rerun it if you think the topics don't make any senseand try it on a new example.Normally, my blog, it's pretty clear.It comes out as tech every time.That's actually the first time it came out as something that,in this characterization, at least, it's a little less clear,somewhere between business and tech.Just one final thing, of course, bear in mind that normallywhen you do topic modeling, you don't know the topics in advance.Here, we did.We used it as a sanity check, but normally, you don't.The whole point is that you'll just see an output like this,and you'll use that to get some insight into the content that we have there.You can think of it as a kind of dimensionality reduction.Also, feel free, of course, to try different values of N.We knew here that there was five topics, but you could try itwith a bigger number and see what happens or you could try it on a sub-corpus.You could try, for example, on the sports documents to seeif they would divide up according to different sport typesor something like that.There it is, topic modeling, very useful technique for dimensionality \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.34139994 0.0636246  0.02555062 0.03400067 0.53542417]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.34139994, 0.0636246 , 0.02555062, 0.03400067, 0.53542417]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_new_example(lda, vectorizer, example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Try a different category  \n",
    "Select a different category from the dataframe, e.g. sport"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Inspect the results  \n",
    "Are they coherent? Do they seem to be different topics?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Try different values of N  \n",
    "Return to Step 1 and repeat the process with a different number of topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Credits to University of London for this Notebook code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
